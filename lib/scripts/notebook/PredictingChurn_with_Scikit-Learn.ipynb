{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, make_scorer, precision_score, recall_score, accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://aws.amazon.com/blogs/machine-learning/predicting-customer-churn-with-amazon-machine-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://amazon-sagemaker.com/datasets/DKD2e_data_sets.zip -O DKD2e_data_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o DKD2e_data_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data sets/churn.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables del dataset\n",
    "- **State:** estado de Estados Unidos en el cual el cliente reside, indicado por una abreviatura de dos caracteres; por ejemplo, OH o NJ\n",
    "- **Account Length:** número de días que la cuenta ha estado activa\n",
    "- **Area Code:** código de área a tres digitos correspondiente al número telefónico del cliente\n",
    "- **Phone:** los 7 digitos restantes del número telefónico del cliente\n",
    "- **Int’l Plan:** indica si el cliente tiene un plan para llamdas internacionales: yes/no\n",
    "- **VMail Plan:** indica si el cliente tiene la funcionalidad de buzón de voz: yes/no\n",
    "- **VMail Message:** promedio de mensajes de voz por mes\n",
    "- **Day Mins:** número total de minutos de llamadas realizadas durante el día\n",
    "- **Day Calls:** número total de llamadas realizadas durante el día\n",
    "- **Day Charge:** costo facturado por las llamadas realizadas durante el día\n",
    "- **Eve Mins:** número total de minutos de llamadas realizadas durante la tarde\n",
    "- **Eve Calls:** número total de llamadas realizadas durante la tarde\n",
    "- **Eve Charge:** costo facturado por las llamadas realizadas durante la tarde\n",
    "- **Night Mins:** número total de minutos de llamadas realizadas durante la noche\n",
    "- **Night Calls:** número total de llamadas realizadas durante la noche\n",
    "- **Night Charge:** costo facturado por las llamadas realizadas durante la noche\n",
    "- **Intl Mins:** número total de minutos de llamadas internacionales realizadas\n",
    "- **Intl Calls:** número total de llamadas internacionales realizadas\n",
    "- **Intl Charge:** costo facturado por las llamadas internacionales realizadas\n",
    "- **CustServ Calls:** número de llamadas realizadas a Servicio al Cliente\n",
    "- **Churn?:** indica si el cliente abandona el servicio o no: true/false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminar caracteres especiales y reemplazar espacios por guiones bajos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [''.join (c if c.isalnum() else '_' for c in str(column)) for column in data.columns]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis exploratorio del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de columnas con las que nos interesa trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['State', 'Account_Length', 'Area_Code', 'Int_l_Plan','VMail_Plan', 'VMail_Message', \n",
    "           'Day_Mins', 'Day_Calls','Eve_Mins', 'Eve_Calls', 'Night_Mins', 'Night_Calls', \n",
    "           'Intl_Mins', 'Intl_Calls', 'CustServ_Calls', 'Churn_']\n",
    "data = data[columns]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entendiendo la distribución de las variables categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'State'\n",
    "fig1, ax = plt.subplots(1,1)\n",
    "x=list(map(str,data[column].value_counts().index))\n",
    "height=data[column].value_counts().values\n",
    "ax.set_title(column)\n",
    "ax.bar(x,height)\n",
    "fig1.set_figheight(3)\n",
    "fig1.set_figwidth(16)\n",
    "\n",
    "columns = ['Area_Code', 'Int_l_Plan', 'VMail_Plan', 'Churn_']\n",
    "fig2, axes = plt.subplots(1,len(columns))\n",
    "\n",
    "for i, column in enumerate(columns, start=0):        \n",
    "    ax = axes[i]\n",
    "    ax.set_title(column)\n",
    "    \n",
    "    x=list(map(str,data[column].value_counts().index))\n",
    "    height=data[column].value_counts().values\n",
    "    ax.bar(x,height)\n",
    "    \n",
    "fig2.set_figheight(3)\n",
    "fig2.set_figwidth(16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entendiendo la distribución de las variables numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Account_Length', 'VMail_Message', 'Day_Mins', 'Day_Calls','Eve_Mins', 'Eve_Calls',\n",
    "           'Night_Mins', 'Night_Calls', 'Intl_Mins', 'Intl_Calls', 'CustServ_Calls']\n",
    "\n",
    "cols=3\n",
    "fig, axes = plt.subplots(round(len(columns)/cols),cols)\n",
    "\n",
    "for i, column in enumerate(columns, start=1): \n",
    "    ax = axes[int((i-1-((i-1)%cols))/cols), (i-1)%cols]\n",
    "    ax.set_title(column)\n",
    "    ax.hist(data[column])\n",
    "\n",
    "if len(columns) < axes.shape[0]*axes.shape[1]:\n",
    "    axes[-1][-1].axis('off')\n",
    "    \n",
    "fig.set_figheight(18)\n",
    "fig.set_figwidth(16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminación del . al final de la palabra False o True en la columna `Churn_` y renombrarla a `Churn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Churn_']=data['Churn_'].str.replace('.','')\n",
    "data.rename(columns={'Churn_':'Churn'}, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding de variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['State','Area_Code']\n",
    "encoder = OneHotEncoder().fit(data[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = encoder.transform(data[columns]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns,axis=1, inplace=True)\n",
    "data = pd.concat([data,pd.DataFrame(transformed, columns=encoder.get_feature_names())],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reemplazar yes/no por 1/0 en columnas Int_l_Plan y VMail_Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Int_l_Plan'] = data['Int_l_Plan'].map(dict(yes=1, no=0))\n",
    "data['VMail_Plan'] = data['VMail_Plan'].map(dict(yes=1, no=0))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reemplazar True/False por 1/0 en columna Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Churn'] = data['Churn'].map({'True': 1, 'False': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separar la etiqueta o target del resto de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data[['Churn']]\n",
    "data.drop(['Churn'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividir los datos para entrenamiento y validación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos que tan balanceado está nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(target['Churn'].value_counts(normalize=True) * 100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Y dividimos en train (80%) y test (20%), manteniendo las mismas proporciones de observaciones por cada clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(data, target, stratify=target, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train: {0} records with clasess: 0={1[0]}% and 1={1[1]}%'.format(train_target.shape[0],\n",
    "                                             round(train_target['Churn'].value_counts(normalize=True) * 100, 1)))\n",
    "\n",
    "print('Test: {0} records with clasess: 0={1[0]}% and 1={1[1]}%'.format(test_target.shape[0],\n",
    "                                             round(test_target['Churn'].value_counts(normalize=True) * 100, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento y selección del mejor modelo\n",
    "\n",
    "Entrenamiento de tres modelos utilizando los siguientes algoritmos\n",
    "- Gradient Boosting\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "Para posteriormente seleccionar el mejor después de haber hecho un fine tuning de cada melo mediante un GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_set, predictions, classes, title):\n",
    "    classes=np.array(classes)\n",
    "    cm = confusion_matrix(test_set, predictions)\n",
    "    \n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, gridspec_kw={'width_ratios': [2, 3]})\n",
    "    im = ax1.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax1.figure.colorbar(im, ax=ax1)\n",
    "    \n",
    "    # We want to show all ticks...\n",
    "    ax1.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=\"Confusion Matrix\",\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax1.get_yticklabels(), rotation=90, ha=\"center\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax1.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "    precision = tp/(fp+tp)\n",
    "    recall = tp/(fn+tp)\n",
    "    specifity = tn/(tn+fp)\n",
    "    \n",
    "    ax2.axis('off')\n",
    "    ax2.text(0,0.9, s='The overall model accuracy is {}% [ACCURACY]'.format(round(accuracy*100,2)), \n",
    "             size='12', ha='left', va='center')\n",
    "    \n",
    "    ax2.text(0,0.7, s='Out of the customers the model predicted as will churn, {}% will actually churn [PRECISION]'.format(round(precision*100,2)), \n",
    "             size='12', ha='left', va='center')\n",
    "    \n",
    "    ax2.text(0,0.5, s='The model will catch {}% of the customers who will actually churn [RECALL / SENSITIVITY]'.format(round(recall*100,2)), \n",
    "             size='12', ha='left', va='center')\n",
    "    \n",
    "    ax2.text(0,0.3, s='The model will catch {}% of the customers who will actually NOT churn [SPECIFITY]'.format(round(specifity*100,2)), \n",
    "             size='12', ha='left', va='center')\n",
    "    \n",
    "    fig.set_figheight(3)\n",
    "    fig.set_figwidth(16)\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuner(clf, dataset, param_grid, scores, splits, refit_score, title):\n",
    "    skf = StratifiedKFold(n_splits=splits)    \n",
    "\n",
    "    grid_search = GridSearchCV(clf, param_grid, scoring=scores, refit=refit_score,\n",
    "                               cv=skf, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "    grid_search.fit(dataset['x_train'], dataset['y_train']['Churn'].tolist())\n",
    "\n",
    "    predictions = grid_search.predict(dataset['x_test'])\n",
    "\n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    labels = ['Not Churn','Churn']\n",
    "    plot_confusion_matrix(dataset['y_test'], predictions, labels, title)\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operating_point_finder(clf, data, target, min_precision):\n",
    "    predict_proba = getattr(clf, 'predict_proba', None)\n",
    "    if callable(predict_proba):\n",
    "        predictions = predict_proba(data)[:, 1]\n",
    "        \n",
    "        precision, recall, thresholds = precision_recall_curve(target, predictions)\n",
    "\n",
    "        operating_point_idx = np.argmax(precision>=min_precision)\n",
    "        \n",
    "        plt.figure(num=None, figsize=(9, 5), facecolor='w', edgecolor='k')\n",
    "        plt.plot(recall, precision)\n",
    "        plt.plot(recall[operating_point_idx], precision[operating_point_idx], 'ro', \n",
    "                 label='Decision threshold: {}'.format(thresholds[operating_point_idx]))\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        return thresholds[operating_point_idx], predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_jobs = -1)\n",
    "\n",
    "parameters = {\n",
    "    'min_samples_split': [3, 5, 10], \n",
    "    'n_estimators' : [100, 200, 300, 400, 500, 700],\n",
    "    'max_depth': [3, 5, 15, 25],\n",
    "    'max_features': [3, 5, 10, 20]\n",
    "}\n",
    "\n",
    "scores = {\n",
    "    'precision_score': make_scorer(precision_score),\n",
    "    'recall_score': make_scorer(recall_score),\n",
    "    'accuracy_score': make_scorer(accuracy_score)\n",
    "}\n",
    "\n",
    "data = {\n",
    "    'x_train': train_data,\n",
    "    'y_train': train_target,\n",
    "    'x_test': test_data,\n",
    "    'y_test': test_target\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = fine_tuner(clf=rf_clf, dataset=data, param_grid=parameters, scores=scores, splits=5, \n",
    "                    refit_score='recall_score', title='Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_threshold, predictions = operating_point_finder(rf_clf, test_data, test_target, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions=[1 if prediction >= decision_threshold else 0 for prediction in predictions]\n",
    "labels = ['Not Churn','Churn']\n",
    "plot_confusion_matrix(test_target, new_predictions, labels, 'Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = GradientBoostingClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.005, 0.001],\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 700],\n",
    "    'min_samples_split': [3, 5, 10], \n",
    "    'max_depth': [3, 5, 15, 25],\n",
    "    'max_features': [3, 5, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_clf = fine_tuner(clf=gb_clf, dataset=data, param_grid=parameters, scores=scores, splits=5, \n",
    "                    refit_score='recall_score', title='Gradient Boosting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_threshold, predictions = operating_point_finder(gb_clf, test_data, test_target, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions=[1 if prediction >= decision_threshold else 0 for prediction in predictions]\n",
    "labels = ['Not Churn','Churn']\n",
    "plot_confusion_matrix(test_target, new_predictions, labels, 'Gradient Boosting')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf = ExtraTreesClassifier(n_jobs = -1)\n",
    "\n",
    "parameters = {\n",
    "    'min_samples_split': [3, 5, 10, 15], \n",
    "    'n_estimators' : [100, 200, 300, 400, 500, 700, 800],\n",
    "    'max_depth': [3, 5, 15, 25],\n",
    "    'max_features': [3, 5, 10, 20, 30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_clf = fine_tuner(clf=et_clf, dataset=data, param_grid=parameters, scores=scores, splits=5, \n",
    "                    refit_score='recall_score', title='Extra Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_threshold, predictions = operating_point_finder(et_clf, test_data, test_target, 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions=[1 if prediction >= decision_threshold else 0 for prediction in predictions]\n",
    "labels = ['Not Churn','Churn']\n",
    "plot_confusion_matrix(test_target, new_predictions, labels, 'Extra Trees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
