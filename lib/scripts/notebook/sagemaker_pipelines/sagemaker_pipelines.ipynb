{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-26 19:20:56--  http://amazon-sagemaker.com/dependencies/dependencies.zip\n",
      "Resolving amazon-sagemaker.com (amazon-sagemaker.com)... 13.227.92.71, 13.227.92.86, 13.227.92.94, ...\n",
      "Connecting to amazon-sagemaker.com (amazon-sagemaker.com)|13.227.92.71|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://amazon-sagemaker.com/dependencies/dependencies.zip [following]\n",
      "--2022-04-26 19:20:56--  https://amazon-sagemaker.com/dependencies/dependencies.zip\n",
      "Connecting to amazon-sagemaker.com (amazon-sagemaker.com)|13.227.92.71|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11904 (12K) [application/zip]\n",
      "Saving to: ‘dependencies.zip’\n",
      "\n",
      "dependencies.zip    100%[===================>]  11.62K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2022-04-26 19:20:56 (6.41 MB/s) - ‘dependencies.zip’ saved [11904/11904]\n",
      "\n",
      "Archive:  dependencies.zip\n",
      "   creating: serving/\n",
      "  inflating: serving/.DS_Store       \n",
      "   creating: serving/custom_inference/\n",
      "  inflating: serving/setup.py        \n",
      "   creating: serving/.ipynb_checkpoints/\n",
      "  inflating: serving/custom_inference/handler.py  \n",
      "  inflating: serving/custom_inference/serving.py  \n",
      "  inflating: serving/custom_inference/__init__.py  \n",
      "   creating: serving/custom_inference/.ipynb_checkpoints/\n",
      "  inflating: serving/.ipynb_checkpoints/setup-checkpoint.py  \n",
      "  inflating: serving/custom_inference/.ipynb_checkpoints/init-checkpoint.py  \n",
      "  inflating: serving/custom_inference/.ipynb_checkpoints/handler-checkpoint.py  \n",
      "  inflating: serving/custom_inference/.ipynb_checkpoints/serving-checkpoint.py  \n"
     ]
    }
   ],
   "source": [
    "!wget http://amazon-sagemaker.com/dependencies/dependencies.zip -O dependencies.zip\n",
    "!unzip -o dependencies.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch/vscode/ch/repos/mlops-sagemaker/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sagemaker\n",
    "import sagemaker_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sagemaker import Session, get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter, CategoricalParameter\n",
    "from sagemaker.inputs import TrainingInput, CreateModelInput, TransformInput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.88.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session()\n",
    "#sagemaker_role = get_execution_role()\n",
    "\n",
    "data_file = 'Data sets/churn.txt'\n",
    "\n",
    "region = session.boto_region_name\n",
    "account_id = session.account_id()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "prefix = 'churn-clf'\n",
    "datasets_prefix = f'{prefix}/datasets'\n",
    "processed_data_prefix = f'{prefix}/processed'\n",
    "eval_prefix = f'{prefix}/eval'\n",
    "transformed_data_prefix = f'{prefix}/transformed'\n",
    "images_directory = f'{prefix}/images'\n",
    "code_prefix = f'{prefix}/code'\n",
    "model_prefix = f'{prefix}/models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'churn-clf/images'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear los contenedores Docker utilizaremos el servicio AWS Code Build y debido a que las imágenes bases serán descargadas del DockerHub repository podríamos llegar a obtener un error indicando que se han realizado demasiadas solicitudes, para conocer más detalles de esta limitante visitar este enlace.\n",
    "\n",
    "Para evitar ese error necesitamos autenticarnos y para esto debemos obtener una cuenta en DockerHub y sustituir usuario y constraseña por los valores correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Secret already exists, updating it...\n",
      "INFO: Secret dockerhub updated\n"
     ]
    }
   ],
   "source": [
    "secret_name = 'dockerhub'\n",
    "sagemaker_utils.create_secret(secret_name,'jagimene','6Doc,.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitaremos un rol de ejecución para ser utilizado en el proyecto de AWS Code Build. Si estamos ejecutando el Notebook con permisos suficientes para crear un rol de IAM, podemos crear el rol simplemente ejecutando el siguiente método, de lo contrario tendría que ser creado de forma manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Role already exists, updating it...\n",
      "INFO: Role updated: CodeBuildExecutionRole\n"
     ]
    }
   ],
   "source": [
    "policy_document={\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [               \n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"ecr:BatchCheckLayerAvailability\",\n",
    "                    \"ecr:CompleteLayerUpload\",\n",
    "                    \"ecr:GetAuthorizationToken\",\n",
    "                    \"ecr:InitiateLayerUpload\",\n",
    "                    \"ecr:PutImage\",\n",
    "                    \"ecr:UploadLayerPart\",\n",
    "                    \"ecr:BatchGetImage\",\n",
    "                    \"ecr:GetDownloadUrlForLayer\",\n",
    "                    \"logs:CreateLogGroup\",\n",
    "                    \"logs:CreateLogStream\",\n",
    "                    \"logs:PutLogEvents\",\n",
    "                    \"s3:PutObject\",\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:GetObjectVersion\",\n",
    "                    \"secretsmanager:GetSecretValue\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "codebuild_role = sagemaker_utils.create_codebuild_execution_role('CodeBuildExecutionRole', policy_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especificamos las dependencias requeridas para cada uno de los contenedores Docker que crearemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_images = {'Processing':{'libraries':{'pandas':'1.2.4',\n",
    "                                            'numpy':'1.20.2',\n",
    "                                            'scikit-learn':'0.24.2'}},\n",
    "                 'Training':{'libraries':{'pandas':'1.2.4',\n",
    "                                          'numpy':'1.20.2',\n",
    "                                          'scikit-learn':'0.24.2',\n",
    "                                          'sagemaker-training':'3.9.2'}},\n",
    "                 'Inference':{'libraries':{'pandas':'1.2.4',\n",
    "                                           'numpy':'1.20.2',\n",
    "                                           'scikit-learn':'0.24.2',\n",
    "                                           'multi-model-server':'1.1.8',                            \n",
    "                                           'sagemaker-inference':'1.5.11',\n",
    "                                           'boto3':'1.21.43',\n",
    "                                           'itsdangerous':'2.0.1'},\n",
    "                              'dependencies':[('serving','/opt/ml/serving')],\n",
    "                              'others':['RUN pip install -e /opt/ml/serving',\n",
    "                                        'LABEL com.amazonaws.sagemaker.capabilities.multi-models=false',\n",
    "                                        'LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true'],\n",
    "                              'entrypoint':['python','/opt/ml/serving/custom_inference/serving.py'],\n",
    "                              'cmd':['serve']}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos y publicamos las imágenes Docker en Amazon Elastic Container Registry para posteriormente poder ser utilizados en los jobs que crearemos y lanzaremos en Amazon SageMaker.\n",
    "\n",
    "Para conocer más detalles sobre el uso de contenedores Docker con Amazon SageMaker y cómo crear tus propios contenedores consultar la documentación ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Repository churn-clf-processing already exists\n",
      "INFO: Repository churn-clf-training already exists\n",
      "INFO: Repository churn-clf-inference already exists\n"
     ]
    }
   ],
   "source": [
    "for image in docker_images:\n",
    "    parameters = {'image_name': f'{prefix}-{image.lower()}',\n",
    "                  'base_image': 'python:3.7.6-slim-buster',\n",
    "                  's3_path': f's3://{bucket}/{images_directory}',\n",
    "                  'role': codebuild_role,  \n",
    "                  'secret': secret_name,\n",
    "                  'wait': False}\n",
    "    \n",
    "    parameters.update(docker_images[image])\n",
    "    \n",
    "    docker_images[image]['build_id'] = sagemaker_utils.create_docker_image(**parameters)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_name': 'churn-clf-inference',\n",
       " 'base_image': 'python:3.7.6-slim-buster',\n",
       " 's3_path': 's3://sagemaker-us-east-1-829825986145/churn-clf/images',\n",
       " 'role': 'arn:aws:iam::829825986145:role/CodeBuildExecutionRole',\n",
       " 'wait': False,\n",
       " 'libraries': {'pandas': '1.2.4',\n",
       "  'numpy': '1.20.2',\n",
       "  'scikit-learn': '0.24.2',\n",
       "  'multi-model-server': '1.1.8',\n",
       "  'sagemaker-inference': '1.5.11',\n",
       "  'boto3': '1.21.43',\n",
       "  'itsdangerous': '2.0.1'},\n",
       " 'dependencies': [('serving', '/opt/ml/serving')],\n",
       " 'others': ['RUN pip install -e /opt/ml/serving',\n",
       "  'LABEL com.amazonaws.sagemaker.capabilities.multi-models=false',\n",
       "  'LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true'],\n",
       " 'entrypoint': ['python', '/opt/ml/serving/custom_inference/serving.py'],\n",
       " 'cmd': ['serve']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que la creación de los containers ocurre de manera asíncrona, esperamos a que termine la creación de los tres contenedores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[Kchurn-clf-processing-build-image................SUCCEEDED!\n",
      "\u001b[Kchurn-clf-inference-build-image.................SUCCEEDED!\n",
      "\u001b[Kchurn-clf-training-build-image..................SUCCEEDED!\n",
      "\u001b[K\u001b[34m✅\u001b[0m Building docker images\n"
     ]
    }
   ],
   "source": [
    "image_uris = sagemaker_utils.wait_for_build([docker_images[image]['build_id'] for image in docker_images])\n",
    "for image in docker_images:\n",
    "    docker_images[image]['image_uri'] = image_uris[docker_images[image]['build_id']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Processing': {'libraries': {'pandas': '1.2.4',\n",
       "   'numpy': '1.20.2',\n",
       "   'scikit-learn': '0.24.2'},\n",
       "  'build_id': 'churn-clf-processing-build-image:7e3a3c1e-4038-4a84-aae6-408606f73789',\n",
       "  'image_uri': '829825986145.dkr.ecr.us-east-1.amazonaws.com/churn-clf-processing:latest'},\n",
       " 'Training': {'libraries': {'pandas': '1.2.4',\n",
       "   'numpy': '1.20.2',\n",
       "   'scikit-learn': '0.24.2',\n",
       "   'sagemaker-training': '3.9.2'},\n",
       "  'build_id': 'churn-clf-training-build-image:acd0f05b-4dba-48d4-85ca-c06c2addfd4e',\n",
       "  'image_uri': '829825986145.dkr.ecr.us-east-1.amazonaws.com/churn-clf-training:latest'},\n",
       " 'Inference': {'libraries': {'pandas': '1.2.4',\n",
       "   'numpy': '1.20.2',\n",
       "   'scikit-learn': '0.24.2',\n",
       "   'multi-model-server': '1.1.8',\n",
       "   'sagemaker-inference': '1.5.11',\n",
       "   'boto3': '1.21.43',\n",
       "   'itsdangerous': '2.0.1'},\n",
       "  'dependencies': [('serving', '/opt/ml/serving')],\n",
       "  'others': ['RUN pip install -e /opt/ml/serving',\n",
       "   'LABEL com.amazonaws.sagemaker.capabilities.multi-models=false',\n",
       "   'LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true'],\n",
       "  'entrypoint': ['python', '/opt/ml/serving/custom_inference/serving.py'],\n",
       "  'cmd': ['serve'],\n",
       "  'build_id': 'churn-clf-inference-build-image:c096f995-2a79-4ea7-8364-0aa4afa43752',\n",
       "  'image_uri': '829825986145.dkr.ecr.us-east-1.amazonaws.com/churn-clf-inference:latest'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docker_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capítulo 4\n",
    "Preparación de los datos\n",
    "Objetivo\n",
    "Antes de poder utilizar Amazon SageMaker para procesar nuestros datos, entrenar un modelo u optimizar algún algoritmo, primero debemos subir los datos a Amazon S3. Es lo que haremos en este capítulo y posteriormente crearemos un Job de Procesamiento en SageMaker el cual nos permitirá realizar las transformaciones necesarias a nuestro dataset como preparación para el entrenamiento de nuestros modelos de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de poder crear el Processing Job para la preparación de los datos para el entrenamiento del modelo, debemos subir los datos a un bucket de Amazon S3. Subiremos el archivo churn.txt localizado en la carpeta Data sets.\n",
    "\n",
    "Antes debe haberse ejecutado el Jupyter Notebook descargado de la sección Introducción ya que este descarga el dataset, de lo contrario nos marcará un error por no encontrar el archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-26 18:35:40--  http://amazon-sagemaker.com/datasets/DKD2e_data_sets.zip\n",
      "Resolving amazon-sagemaker.com (amazon-sagemaker.com)... 13.227.92.115, 13.227.92.71, 13.227.92.86, ...\n",
      "Connecting to amazon-sagemaker.com (amazon-sagemaker.com)|13.227.92.115|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://amazon-sagemaker.com/datasets/DKD2e_data_sets.zip [following]\n",
      "--2022-04-26 18:35:41--  https://amazon-sagemaker.com/datasets/DKD2e_data_sets.zip\n",
      "Connecting to amazon-sagemaker.com (amazon-sagemaker.com)|13.227.92.115|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1003616 (980K) [application/zip]\n",
      "Saving to: ‘DKD2e_data_sets.zip’\n",
      "\n",
      "DKD2e_data_sets.zip 100%[===================>] 980.09K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-04-26 18:35:41 (7.30 MB/s) - ‘DKD2e_data_sets.zip’ saved [1003616/1003616]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://amazon-sagemaker.com/datasets/DKD2e_data_sets.zip -O DKD2e_data_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  DKD2e_data_sets.zip\n",
      " extracting: Data sets/adult.zip     \n",
      "  inflating: Data sets/cars.txt      \n",
      "  inflating: Data sets/cars2.txt     \n",
      "  inflating: Data sets/cereals.CSV   \n",
      "  inflating: Data sets/churn.txt     \n",
      "  inflating: Data sets/ClassifyRisk  \n",
      "  inflating: Data sets/ClassifyRisk - Missing.txt  \n",
      " extracting: Data sets/DKD2e data sets.zip  \n",
      "  inflating: Data sets/nn1.txt       \n"
     ]
    }
   ],
   "source": [
    "!unzip -o DKD2e_data_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Data sets/churn.txt',\n",
       " 'sagemaker-us-east-1-829825986145',\n",
       " 'churn-clf/datasets')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file, bucket, datasets_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 438k/438k [00:01<00:00, 239kB/s]\n"
     ]
    }
   ],
   "source": [
    "data_s3_path = sagemaker_utils.upload(data_file, f's3://{bucket}/{datasets_prefix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_role = \"arn:aws:iam::829825986145:role/service-role/AmazonSageMaker-ExecutionRole-20220424T173630\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script para procesamiento\n",
    "Para crear un Job de procesamiento de Amazon SageMaker primero crearemos un script python el cual nombraremos processing.py y tendrá toda la lógica necesaria para realizar las mismas transformaciones que en el Jupyter Notebook de ejemplo descargado en la Introducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_script_file = 'code/data_prep.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_utils.make_dirs(data_prep_script_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor(\n",
    "    image_uri=docker_images['Processing']['image_uri'],\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    entrypoint=['python3',f'/opt/ml/processing/input/code/{os.path.basename(data_prep_script_file)}'],\n",
    "    volume_size_in_gb=5,\n",
    "    max_runtime_in_seconds=60*60*2)# dos horas "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4989b284843198940c9466d0e8ffb2ed3dc06eb1954444794d5f5c9e3a6f8233"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
