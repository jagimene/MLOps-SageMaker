{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se realiza el despliegue de modelos con SageMaker\n",
    "======================================================\n",
    "\n",
    "![Despliegue de modelo](img/Deployment.png?width=50pc)  \n",
    "\n",
    "Desplegar un modelo con **Amazon SageMaker** consiste en tres pasos:\n",
    "\n",
    "1.  **Crear un modelo en SageMaker** – al crear un modelo, se le especifica a **Amazon SageMaker** en dónde encontrar los componentes del modelo. Esto incluye la ruta del _bucket_ de **Amazon S3** en dónde los artefactos del modelo se encuentran almacenados. Estos deben estar empaquetados en un archivo con nombre `model.tar.gz`. Así como la ruta de **Amazon Elastic Container Registry** en dónde se encuentra la imagen Docker que contiene el código para la inferencia\n",
    "2.  **Crear una configuración para el endpoint** – se especifíca el nombre del modelo a utilizar y las instancias de cómputo para Machine Learning que se desea que **Amazon SageMaker** lance para hostear el modelo. Es posible configurar el endpoint para que automáticamente escale el número de instancias aprovisionadas. Cuando se especifican dos o más instancias, **Amazon SageMaker** las lanza en _Zonas de Disponibilidad_ distintas y gestiona el reemplazo de las instancias cuando es necesario, esto asegura una disponibilidad continua\n",
    "3.  **Crear un endpoint HTTPS** – a través de la configuración del endpoint proporcionada, **Amazon SageMaker** aprovisiona las instancias de cómputo requeridas y despliega el modelo conforme a la especificación de la configuración. Para obtener predicciones, la aplicación cliente puede realizar una petición al endpoint a través del protocolo HTTPS.\n",
    "\n",
    "Para mayor información consultar la [documentación](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despliegue en SageMaker\n",
    "=======================\n",
    "\n",
    "Para hacer el deployment del modelo seleccionado basta con invocar el método `deploy()` del objeto `Estimator`, del proceso de entrenamiento que mejor desempeño tuvo para ese algoritmo, proporcionando los siguientes parámetros:\n",
    "\n",
    "*   `endpoint_name` – nombre del endpoint a crear\n",
    "*   `initial_instance_count` – número inicial de instancias a utilizar en el clúster del endpoint\n",
    "*   `instance_type` – tipo de instancia(s) a utilizar\n",
    "*   `image_uri` – imagen Docker a utilizar. En este caso la creada para el despliegue del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = f'{prefix}-best-model-{strftime(\"%M-%S\", gmtime())}'\n",
    "    \n",
    "    best_estimator = tuners[best_model_found['Algorithm']].best_estimator()\n",
    "    \n",
    "    predictor = best_estimator.deploy(endpoint_name=endpoint_name,\n",
    "                                      initial_instance_count=1, \n",
    "                                      instance_type='ml.m5.large',\n",
    "                                      entry_point = os.path.basename(training_script_file),\n",
    "                                      source_dir = training_script_path,\n",
    "                                      image_uri=docker_images['Inference']['image_uri'])\n",
    "    \n",
    "    print(f'\\nEndpoint Name: {endpoint_name}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ejecución de esta celda puede demorar varios minutos\n",
    "\n",
    "Una vez obteniendo el endpoint listo, podemos realizar peticiones para obtener predicciones."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
