{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script para procesamiento\n",
    "Para crear un Job de procesamiento de Amazon SageMaker primero crearemos un script python el cual nombraremos processing.py y tendrá toda la lógica necesaria para realizar las mismas transformaciones que en el Jupyter Notebook de ejemplo descargado en la Introducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch/vscode/ch/repos/mlops-sagemaker/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sagemaker\n",
    "import sagemaker_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sagemaker import Session, get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter, CategoricalParameter\n",
    "from sagemaker.inputs import TrainingInput, CreateModelInput, TransformInput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "sagemaker.__version__\n",
    "session = Session()\n",
    "#sagemaker_role = get_execution_role()\n",
    "\n",
    "data_file = 'Data sets/churn.txt'\n",
    "\n",
    "region = session.boto_region_name\n",
    "account_id = session.account_id()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "prefix = 'churn-clf'\n",
    "datasets_prefix = f'{prefix}/datasets'\n",
    "processed_data_prefix = f'{prefix}/processed'\n",
    "eval_prefix = f'{prefix}/eval'\n",
    "transformed_data_prefix = f'{prefix}/transformed'\n",
    "images_directory = f'{prefix}/images'\n",
    "code_prefix = f'{prefix}/code'\n",
    "model_prefix = f'{prefix}/models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_script_file = 'code/data_prep.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $data_prep_script_file\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def to_pkl(data, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    script_name = os.path.basename(__file__)\n",
    "    \n",
    "    print(f'INFO: {script_name}: Iniciando la preparación de los datos')\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--test-size', type=float, default=0.1)\n",
    "    parser.add_argument('--data-file', type=str, default='train.csv')\n",
    "    parser.add_argument('--train-data-file', type=str)\n",
    "    parser.add_argument('--train-target-file', type=str)\n",
    "    parser.add_argument('--test-data-file', type=str)\n",
    "    parser.add_argument('--test-target-file', type=str)\n",
    "    parser.add_argument('--encoder-file', type=str)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()    \n",
    "    \n",
    "    print(f'INFO: {script_name}: Parámetros recibidos: {args}')\n",
    "    \n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    data_path = os.path.join(input_path, args.data_file) \n",
    "    \n",
    "    # Cargar dataset\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    # Eliminar caracteres especiales y reemplazar espacios por guiones bajos\n",
    "    data.columns = [''.join (c if c.isalnum() else '_' for c in str(column)) for column in data.columns]\n",
    "    \n",
    "    # Selección de columnas\n",
    "    columns = ['State', 'Account_Length', 'Area_Code', 'Int_l_Plan','VMail_Plan', 'VMail_Message', \n",
    "           'Day_Mins', 'Day_Calls','Eve_Mins', 'Eve_Calls', 'Night_Mins', 'Night_Calls', \n",
    "           'Intl_Mins', 'Intl_Calls', 'CustServ_Calls', 'Churn_']\n",
    "    data = data[columns]\n",
    "    \n",
    "    # Eliminación del . al final de la palabra False o True en la columna Churn_ y renombrarla a Churn\n",
    "    data['Churn_']=data['Churn_'].str.replace('.','')\n",
    "    data.rename(columns={'Churn_':'Churn'}, inplace=True)\n",
    "    \n",
    "    # One hot encoding de variables categóricas\n",
    "    columns = ['State','Area_Code']\n",
    "    encoder = OneHotEncoder().fit(data[columns])\n",
    "    \n",
    "    transformed = encoder.transform(data[columns]).toarray()\n",
    "    \n",
    "    data.drop(columns,axis=1, inplace=True)\n",
    "    data = pd.concat([data,pd.DataFrame(transformed, columns=encoder.get_feature_names())],axis=1)\n",
    "    \n",
    "    # Reemplazar yes/no por 1/0 en columnas Int_l_Plan y VMail_Plan\n",
    "    data['Int_l_Plan'] = data['Int_l_Plan'].map(dict(yes=1, no=0))\n",
    "    data['VMail_Plan'] = data['VMail_Plan'].map(dict(yes=1, no=0))\n",
    "    \n",
    "    # Reemplazar True/False por 1/0 en columna Churn\n",
    "    data['Churn'] = data['Churn'].map({'True': 1, 'False': 0})\n",
    "    \n",
    "    # Separar la etiqueta o target del resto de los datos\n",
    "    target = data[['Churn']]\n",
    "    data.drop(['Churn'], axis=1, inplace=True)\n",
    "    \n",
    "    # Y dividimos en train (80%) y test (20%), manteniendo las mismas proporciones de observaciones por cada clase\n",
    "    train_data, test_data, train_target, test_target = train_test_split(data, target, stratify=target, \n",
    "                                                                        test_size=args.test_size)\n",
    "    \n",
    "    print('Train: {0} records with clasess: 0={1[0]}% and 1={1[1]}%'.format(train_target.shape[0],\n",
    "                                             round(train_target['Churn'].value_counts(normalize=True) * 100, 1)))\n",
    "\n",
    "    print('Test: {0} records with clasess: 0={1[0]}% and 1={1[1]}%'.format(test_target.shape[0],\n",
    "                                             round(test_target['Churn'].value_counts(normalize=True) * 100, 1)))\n",
    "    \n",
    "    # Guardar los dataframes resultantes y el encoder\n",
    "    train_data.to_csv(os.path.join(output_path, 'train_data', args.train_data_file), index=False)\n",
    "    train_target.to_csv(os.path.join(output_path, 'train_target', args.train_target_file), index=False)\n",
    "    test_data.to_csv(os.path.join(output_path, 'test_data', args.test_data_file), index=False)\n",
    "    test_target.to_csv(os.path.join(output_path, 'test_target', args.test_target_file), index=False)\n",
    "    to_pkl(encoder, os.path.join(output_path, 'encoder', args.encoder_file))\n",
    "    \n",
    "    print(f'INFO: {script_name}: Finalizando la preparación de los datos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y subimos el script creado a un bucket de Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 4.10k/4.10k [00:01<00:00, 3.74kB/s]\n"
     ]
    }
   ],
   "source": [
    "data_prep_script_path = sagemaker_utils.upload(data_prep_script_file, f's3://{bucket}/{code_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al inicio del código podemos observar que se definen los siguientes parámetros, los cuales se recibirán como argumentos de la línea de comandos y de esta forma nos permitirá usar estos valores como mejor nos convenga en nuestro programa  \n",
    "\n",
    "Adicionalmente vemos las rutas que utilizamos para cargar el dataset (archvio churn.txt) y posteriormente guardar los DataFrames y encoder creado  \n",
    "\n",
    "Y por último, podemos observar que la lógica que hemos incorporado en el script para la preparación de los datos es prácticamente la misma que la del Jupyter Notebook descargado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4989b284843198940c9466d0e8ffb2ed3dc06eb1954444794d5f5c9e3a6f8233"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
