{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script para procesamiento\n",
    "Para crear un Job de procesamiento de Amazon SageMaker primero crearemos un script python el cual nombraremos processing.py y tendrá toda la lógica necesaria para realizar las mismas transformaciones que en el Jupyter Notebook de ejemplo descargado en la Introducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch/vscode/ch/repos/mlops-sagemaker/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sagemaker\n",
    "import sagemaker_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import gmtime, strftime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sagemaker import Session, get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter, CategoricalParameter\n",
    "from sagemaker.inputs import TrainingInput, CreateModelInput, TransformInput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "sagemaker.__version__\n",
    "session = Session()\n",
    "#sagemaker_role = get_execution_role()\n",
    "\n",
    "data_file = 'Data sets/churn.txt'\n",
    "\n",
    "region = session.boto_region_name\n",
    "account_id = session.account_id()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "prefix = 'churn-clf'\n",
    "datasets_prefix = f'{prefix}/datasets'\n",
    "processed_data_prefix = f'{prefix}/processed'\n",
    "eval_prefix = f'{prefix}/eval'\n",
    "transformed_data_prefix = f'{prefix}/transformed'\n",
    "images_directory = f'{prefix}/images'\n",
    "code_prefix = f'{prefix}/code'\n",
    "model_prefix = f'{prefix}/models'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_script_file = 'code/data_prep.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing $data_prep_script_file\n"
     ]
    }
   ],
   "source": [
    "%%writefile $data_prep_script_file\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def to_pkl(data, file):\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    script_name = os.path.basename(__file__)\n",
    "    \n",
    "    print(f'INFO: {script_name}: Iniciando la preparación de los datos')\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--test-size', type=float, default=0.1)\n",
    "    parser.add_argument('--data-file', type=str, default='train.csv')\n",
    "    parser.add_argument('--train-data-file', type=str)\n",
    "    parser.add_argument('--train-target-file', type=str)\n",
    "    parser.add_argument('--test-data-file', type=str)\n",
    "    parser.add_argument('--test-target-file', type=str)\n",
    "    parser.add_argument('--encoder-file', type=str)\n",
    "    \n",
    "    args, _ = parser.parse_known_args()    \n",
    "    \n",
    "    print(f'INFO: {script_name}: Parámetros recibidos: {args}')\n",
    "    \n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    data_path = os.path.join(input_path, args.data_file) \n",
    "    \n",
    "    # Cargar dataset\n",
    "    data = pd.read_csv(data_path)\n",
    "    \n",
    "    # Eliminar caracteres especiales y reemplazar espacios por guiones bajos\n",
    "    data.columns = [''.join (c if c.isalnum() else '_' for c in str(column)) for column in data.columns]\n",
    "    \n",
    "    # Selección de columnas\n",
    "    columns = ['State', 'Account_Length', 'Area_Code', 'Int_l_Plan','VMail_Plan', 'VMail_Message', \n",
    "           'Day_Mins', 'Day_Calls','Eve_Mins', 'Eve_Calls', 'Night_Mins', 'Night_Calls', \n",
    "           'Intl_Mins', 'Intl_Calls', 'CustServ_Calls', 'Churn_']\n",
    "    data = data[columns]\n",
    "    \n",
    "    # Eliminación del . al final de la palabra False o True en la columna Churn_ y renombrarla a Churn\n",
    "    data['Churn_']=data['Churn_'].str.replace('.','')\n",
    "    data.rename(columns={'Churn_':'Churn'}, inplace=True)\n",
    "    \n",
    "    # One hot encoding de variables categóricas\n",
    "    columns = ['State','Area_Code']\n",
    "    encoder = OneHotEncoder().fit(data[columns])\n",
    "    \n",
    "    transformed = encoder.transform(data[columns]).toarray()\n",
    "    \n",
    "    data.drop(columns,axis=1, inplace=True)\n",
    "    data = pd.concat([data,pd.DataFrame(transformed, columns=encoder.get_feature_names())],axis=1)\n",
    "    \n",
    "    # Reemplazar yes/no por 1/0 en columnas Int_l_Plan y VMail_Plan\n",
    "    data['Int_l_Plan'] = data['Int_l_Plan'].map(dict(yes=1, no=0))\n",
    "    data['VMail_Plan'] = data['VMail_Plan'].map(dict(yes=1, no=0))\n",
    "    \n",
    "    # Reemplazar True/False por 1/0 en columna Churn\n",
    "    data['Churn'] = data['Churn'].map({'True': 1, 'False': 0})\n",
    "    \n",
    "    # Separar la etiqueta o target del resto de los datos\n",
    "    target = data[['Churn']]\n",
    "    data.drop(['Churn'], axis=1, inplace=True)\n",
    "    \n",
    "    # Y dividimos en train (80%) y test (20%), manteniendo las mismas proporciones de observaciones por cada clase\n",
    "    train_data, test_data, train_target, test_target = train_test_split(data, target, stratify=target, \n",
    "                                                                        test_size=args.test_size)\n",
    "    \n",
    "    print('Train: {0} records with clasess: 0={1[0]}% and 1={1[1]}%'.format(train_target.shape[0],\n",
    "                                             round(train_target['Churn'].value_counts(normalize=True) * 100, 1)))\n",
    "\n",
    "    print('Test: {0} records with clasess: 0={1[0]}% and 1={1[1]}%'.format(test_target.shape[0],\n",
    "                                             round(test_target['Churn'].value_counts(normalize=True) * 100, 1)))\n",
    "    \n",
    "    # Guardar los dataframes resultantes y el encoder\n",
    "    train_data.to_csv(os.path.join(output_path, 'train_data', args.train_data_file), index=False)\n",
    "    train_target.to_csv(os.path.join(output_path, 'train_target', args.train_target_file), index=False)\n",
    "    test_data.to_csv(os.path.join(output_path, 'test_data', args.test_data_file), index=False)\n",
    "    test_target.to_csv(os.path.join(output_path, 'test_target', args.test_target_file), index=False)\n",
    "    to_pkl(encoder, os.path.join(output_path, 'encoder', args.encoder_file))\n",
    "    \n",
    "    print(f'INFO: {script_name}: Finalizando la preparación de los datos')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y subimos el script creado a un bucket de Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 4.10k/4.10k [00:01<00:00, 3.87kB/s]\n"
     ]
    }
   ],
   "source": [
    "data_prep_script_path = sagemaker_utils.upload(data_prep_script_file, f's3://{bucket}/{code_prefix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especificamos las dependencias requeridas para cada uno de los contenedores Docker que crearemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_role = \"arn:aws:iam::829825986145:role/service-role/AmazonSageMaker-ExecutionRole-20220424T173630\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al inicio del código podemos observar que se definen los siguientes parámetros, los cuales se recibirán como argumentos de la línea de comandos y de esta forma nos permitirá usar estos valores como mejor nos convenga en nuestro programa  \n",
    "\n",
    "Adicionalmente vemos las rutas que utilizamos para cargar el dataset (archvio churn.txt) y posteriormente guardar los DataFrames y encoder creado  \n",
    "\n",
    "Y por último, podemos observar que la lógica que hemos incorporado en el script para la preparación de los datos es prácticamente la misma que la del Jupyter Notebook descargado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Esta ejecucion se debe hacer posterior a la generacion de las imagenes, para tener las img uris**  \n",
    "La traemos de Sagemaker_pipelines previamente ejecutado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_images = {'Processing': {'libraries': {'pandas': '1.2.4',\n",
    "   'numpy': '1.20.2',\n",
    "   'scikit-learn': '0.24.2'},\n",
    "  'build_id': 'churn-clf-processing-build-image:7e3a3c1e-4038-4a84-aae6-408606f73789',\n",
    "  'image_uri': '829825986145.dkr.ecr.us-east-1.amazonaws.com/churn-clf-processing:latest'},\n",
    " 'Training': {'libraries': {'pandas': '1.2.4',\n",
    "   'numpy': '1.20.2',\n",
    "   'scikit-learn': '0.24.2',\n",
    "   'sagemaker-training': '3.9.2'},\n",
    "  'build_id': 'churn-clf-training-build-image:acd0f05b-4dba-48d4-85ca-c06c2addfd4e',\n",
    "  'image_uri': '829825986145.dkr.ecr.us-east-1.amazonaws.com/churn-clf-training:latest'},\n",
    " 'Inference': {'libraries': {'pandas': '1.2.4',\n",
    "   'numpy': '1.20.2',\n",
    "   'scikit-learn': '0.24.2',\n",
    "   'multi-model-server': '1.1.8',\n",
    "   'sagemaker-inference': '1.5.11',\n",
    "   'boto3': '1.21.43',\n",
    "   'itsdangerous': '2.0.1'},\n",
    "  'dependencies': [('serving', '/opt/ml/serving')],\n",
    "  'others': ['RUN pip install -e /opt/ml/serving',\n",
    "   'LABEL com.amazonaws.sagemaker.capabilities.multi-models=false',\n",
    "   'LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true'],\n",
    "  'entrypoint': ['python', '/opt/ml/serving/custom_inference/serving.py'],\n",
    "  'cmd': ['serve'],\n",
    "  'build_id': 'churn-clf-inference-build-image:c096f995-2a79-4ea7-8364-0aa4afa43752',\n",
    "  'image_uri': '829825986145.dkr.ecr.us-east-1.amazonaws.com/churn-clf-inference:latest'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor(\n",
    "    image_uri=docker_images['Processing']['image_uri'],\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    entrypoint=['python3',f'/opt/ml/processing/input/code/{os.path.basename(data_prep_script_file)}'],\n",
    "    volume_size_in_gb=5,\n",
    "    max_runtime_in_seconds=60*60*2)# dos horas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previo a ejecutar nuestro Job de procesamiento, definimos las siguientes variables para mas adelante poder re-utilizarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = 'train_data.csv'\n",
    "train_target_file = 'train_target.csv'\n",
    "test_data_file = 'test_data.csv'\n",
    "test_target_file = 'test_target.csv'\n",
    "encoder_file = 'encoder.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y finalmente ejecutamos el Job utilizando el metodo **run** del objeto creado mediante la clase **Processor**.  \n",
    "Debemos pasar las rutas de los buckets de Amazon S3 tanto para **inputs** (entradas) como para **outputs** (salidas). De esta forma SageMaker sabe de dónde tomar los datos de entrada y en dónde colocar los archivos resultantes de ejecutar el Job de procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  churn-clf-processing-2022-05-01-17-44-57-933\n",
      "Inputs:  [{'InputName': 'input', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-829825986145/churn-clf/datasets', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-829825986145/churn-clf/code/data_prep.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-829825986145/churn-clf/processed/train_data', 'LocalPath': '/opt/ml/processing/output/train_data', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'train_target', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-829825986145/churn-clf/processed/train_target', 'LocalPath': '/opt/ml/processing/output/train_target', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-829825986145/churn-clf/processed/test_data', 'LocalPath': '/opt/ml/processing/output/test_data', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_target', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-829825986145/churn-clf/processed/test_target', 'LocalPath': '/opt/ml/processing/output/test_target', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'encoder', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-829825986145/churn-clf/processed/encoder', 'LocalPath': '/opt/ml/processing/output/encoder', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".....................\n",
      "INFO: data_prep.py: Iniciando la preparación de los datos\n",
      "INFO: data_prep.py: Parámetros recibidos: Namespace(data_file='churn.txt', encoder_file='encoder.pkl', test_data_file='test_data.csv', test_size=0.1, test_target_file='test_target.csv', train_data_file='train_data.csv', train_target_file='train_target.csv')\n",
      "/opt/ml/processing/input/code/data_prep.py:48: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will*not* be treated as literal strings when regex=True.\n",
      "  data['Churn_']=data['Churn_'].str.replace('.','')\n",
      "Train: 2999 records with clasess: 0=85.5% and 1=14.5%\n",
      "Test: 334 records with clasess: 0=85.6% and 1=14.4%\n",
      "INFO: data_prep.py: Finalizando la preparación de los datos\n"
     ]
    }
   ],
   "source": [
    "data_prep_parameters = {\n",
    "    'inputs':[ProcessingInput(input_name='input',\n",
    "                    source=f's3://{bucket}/{datasets_prefix}',\n",
    "                    destination='/opt/ml/processing/input'),\n",
    "              ProcessingInput(input_name='code',\n",
    "                    source=data_prep_script_path,\n",
    "                    destination='/opt/ml/processing/input/code')],\n",
    "    'outputs':[ProcessingOutput(output_name='train_data',\n",
    "                    source=f'/opt/ml/processing/output/train_data',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/train_data'),\n",
    "               ProcessingOutput(output_name='train_target',\n",
    "                    source=f'/opt/ml/processing/output/train_target',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/train_target'),\n",
    "               ProcessingOutput(output_name='test_data',\n",
    "                    source=f'/opt/ml/processing/output/test_data',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/test_data'),\n",
    "               ProcessingOutput(output_name='test_target',\n",
    "                    source=f'/opt/ml/processing/output/test_target',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/test_target'),\n",
    "               ProcessingOutput(output_name='encoder',\n",
    "                    source=f'/opt/ml/processing/output/encoder',\n",
    "                    destination=f's3://{bucket}/{processed_data_prefix}/encoder')],\n",
    "    'arguments':['--test-size', '0.1',\n",
    "                 '--data-file', 'churn.txt',\n",
    "                 '--train-data-file', train_data_file,\n",
    "                 '--train-target-file', train_target_file,\n",
    "                 '--test-data-file', test_data_file,\n",
    "                 '--test-target-file', test_target_file,\n",
    "                 '--encoder-file', encoder_file]}\n",
    "\n",
    "processor.run(**data_prep_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-829825986145/churn-clf/processed/train_data'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_utils.get_processor_output_path(processor, 'train_data')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4989b284843198940c9466d0e8ffb2ed3dc06eb1954444794d5f5c9e3a6f8233"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
