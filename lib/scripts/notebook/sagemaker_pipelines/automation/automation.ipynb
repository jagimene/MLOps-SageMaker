{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capítulo 10\n",
    "\n",
    "Automatización del Pipeline\n",
    "===========================\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Integrar todos los componentes previamente construidos en un workflow orquestado por **AWS Step Functions** y que permita ejecutar todo el pipeline sin necesidad de depender del Jupyter Notebook.\n",
    "\n",
    "Una vez que terminemos, el resultado será el pipeline que se muestra en la animación, la representación gráfica es generada por **AWS Step Functions**. Lo crearemos reutilizando los componentes previamente creados.\n",
    "\n",
    "![Pipeline](img/Pipeline.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo funciona Step Functions\n",
    "============================\n",
    "\n",
    "AWS Step Functions permite coordinar múltiples servicios AWS en un workflow _serverless_. Utilizando **AWS Step Functions**, es posible ejecutar workflows que combinen servicios como **Amazon SageMaker** y **AWS Lambda**, entre otros. Los workflows constan de _pasos_, en los cuales la salida de un _paso_ se convierte en la entrada del siguiente.\n",
    "\n",
    "El SDK de Step Functions para Data Science permite crear y ejecutar workflows de Machine Learning con **AWS Step Functions** directamente desde Python, y desde un Jupyter Notebook. Cuando la definición del workflow esta terminada, es subido al servicio **AWS Step Functions** para su ejecución en el Cloud. De esa forma, una vez creado o actualizado el workflow, este vive en el Cloud y puede ser reutilizado.\n",
    "\n",
    "El workflow puede ser ejecutado tantas veces como sea necesario, y opcionalmente es posible cambiar la entrada de datos del workflow en cada ejecución. Cada que el workflow es ejecutado, este crea una nueva instancia de ejecución en el Cloud. Es posible ejecutarlo varias veces en paralelo.\n",
    "\n",
    "A través del SDK es posible crear los pasos, encadenarlos juntos para un workflow, crear el workflow en **AWS StepFunctions**, y ejecutar el workflow en AWS cloud.\n",
    "\n",
    "![Step Functions](img/StepFunctions1.png?width=50pc)\n",
    "\n",
    "Una vez en ejecución, puede ser inspeccionado el progreso mediante comandos SDK o a través de la consola web de administración.\n",
    "\n",
    "![Step Functions](img/StepFunctions2.png?width=40pc)\n",
    "\n",
    "Utilizaremos **AWS Step Functions** para crear un worflow que nos permita integrar y automatizar todos los pasos antes creados en **Amazon SageMaker**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de Roles\n",
    "=================\n",
    "\n",
    "Lo primero que tenemos que hacer antes de crear el workflow para automatizar el pipeline, es crear **dos** [roles](https://docs.aws.amazon.com/es_es/IAM/latest/UserGuide/id_roles.html) que serán utilizados por los servicios **AWS Step Functions** y **AWS Lambda**.\n",
    "\n",
    "Empezamos por importar las dependencias que utilizaremos para la definición y ejecución del workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-02 19:15:21--  http://amazon-sagemaker.com/dependencies/utils.py\n",
      "Resolving amazon-sagemaker.com (amazon-sagemaker.com)... 13.227.92.115, 13.227.92.71, 13.227.92.94, ...\n",
      "Connecting to amazon-sagemaker.com (amazon-sagemaker.com)|13.227.92.115|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://amazon-sagemaker.com/dependencies/utils.py [following]\n",
      "--2022-05-02 19:15:22--  https://amazon-sagemaker.com/dependencies/utils.py\n",
      "Connecting to amazon-sagemaker.com (amazon-sagemaker.com)|13.227.92.115|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6039 (5.9K) [text/x-python]\n",
      "Saving to: ‘utils.py.4’\n",
      "\n",
      "utils.py.4          100%[===================>]   5.90K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-05-02 19:15:22 (720 MB/s) - ‘utils.py.4’ saved [6039/6039]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://amazon-sagemaker.com/dependencies/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ch/vscode/ch/repos/mlops-sagemaker/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import stepfunctions\n",
    "import logging\n",
    "import time\n",
    "import sagemaker\n",
    "import sagemaker_utils\n",
    "import utils\n",
    "\n",
    "from stepfunctions import steps\n",
    "from stepfunctions.steps import TrainingStep, ModelStep\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sagemaker import Session, get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.tuner import HyperparameterTuner, ContinuousParameter, IntegerParameter, CategoricalParameter\n",
    "from sagemaker.inputs import TrainingInput, CreateModelInput, TransformInput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep, CreateModelStep, TransformStep\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "\n",
    "from sagemaker import Session\n",
    "from sagemaker.s3 import S3Uploader\n",
    "session = Session()\n",
    "sess = session\n",
    "\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)\n",
    "\n",
    "sagemaker_role = \"arn:aws:iam::829825986145:role/service-role/AmazonSageMaker-ExecutionRole-20220424T173630\"\n",
    "\n",
    "region = session.boto_region_name\n",
    "account_id = session.account_id()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "prefix = 'churn-clf'\n",
    "datasets_prefix = f'{prefix}/datasets'\n",
    "processed_data_prefix = f'{prefix}/processed'\n",
    "eval_prefix = f'{prefix}/eval'\n",
    "transformed_data_prefix = f'{prefix}/transformed'\n",
    "images_directory = f'{prefix}/images'\n",
    "code_prefix = f'{prefix}/code'\n",
    "model_prefix = f'{prefix}/models'\n",
    "\n",
    "#processing\n",
    "train_data_file = 'train_data.csv'\n",
    "train_target_file = 'train_target.csv'\n",
    "test_data_file = 'test_data.csv'\n",
    "test_target_file = 'test_target.csv'\n",
    "encoder_file = 'encoder.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y definimos algunas variables con nombres que más adelante utilizaremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_role_name = '{}-StepFunctionsWorkflowExecutionRole'.format(prefix) \n",
    "lambda_role_name = '{}-LambdaExecutionRole'.format(prefix)\n",
    "model_function_name = '{}-ModelFunction'.format(prefix)\n",
    "select_model_function_name = '{}-SelectModelFunction'.format(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación definimos la política de permisos para el _Rol de Ejecución_ para **AWS Step Functions**, es decir para el workflow como tal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_functions_policy_document={\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [           \n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"iam:PassRole\",\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"states:*\",\n",
    "                \"sagemaker:*\",\n",
    "                \"lambda:*\",\n",
    "                \"cloudwatch:*\",\n",
    "                \"events:*\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_functions_asume_role_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Principal\": {\n",
    "            \"Service\": \"states.amazonaws.com\"\n",
    "        },\n",
    "        \"Action\": \"sts:AssumeRole\"\n",
    "    }]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y utilizando el siguiente método para crear el _Rol de Ejecución_ para **AWS Step Functions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Role already exists, updating it...\n",
      "INFO: Role updated: churn-clf-StepFunctionsWorkflowExecutionRole\n"
     ]
    }
   ],
   "source": [
    "workflow_role = utils.create_or_update_iam_role(role_name = workflow_role_name, \n",
    "                          role_desc = 'Execution role for Step Functions workflow', \n",
    "                          asume_role_policy_document = step_functions_asume_role_document,\n",
    "                          policy_name = 'StepFunctionsWorkflowExecutionPolicy',\n",
    "                          policy_document = step_functions_policy_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Siguiendo el mismo procedimiento creamos el _Rol de Ejecución_ para **AWS Lambda**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:*\",\n",
    "                \"s3:*\",\n",
    "                \"states:StartExecution\",\n",
    "                \"iam:PassRole\",\n",
    "                \"logs:CreateLogGroup\",\n",
    "                \"logs:CreateLogStream\",\n",
    "                \"logs:PutLogEvents\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_asume_role_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [{\n",
    "        \"Effect\": \"Allow\",\n",
    "        \"Principal\": {\n",
    "            \"Service\": \"lambda.amazonaws.com\"\n",
    "        },\n",
    "        \"Action\": \"sts:AssumeRole\"\n",
    "    }]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Role already exists, updating it...\n",
      "INFO: Role updated: churn-clf-LambdaExecutionRole\n"
     ]
    }
   ],
   "source": [
    "lambda_role = utils.create_or_update_iam_role(role_name = lambda_role_name, \n",
    "                                        role_desc = 'Execution role for Lambda functions', \n",
    "                                        asume_role_policy_document = lambda_asume_role_document,\n",
    "                                        policy_name = 'LambdaExecutionPolicy',\n",
    "                                        policy_document = lambda_policy_document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado los roles, estamos listos para iniciar la definición del workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetros del workflow\n",
    "=======================\n",
    "\n",
    "Empezamos por definir los parámetros de entrada que recibirá el workflow, estos parámetros podremos posteriormente utilizarlos en la ejecución de los pasos del workflow y de esta forma parametrizar algunas cosas y volver más flexible nuestro pipeline. Esto lo hacemos mediante el uso de la función [ExecutionInput()](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/placeholders.html?highlight=ExecutionInput#stepfunctions.inputs.ExecutionInput) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(schema={\n",
    "        'ProcessingJobName': str,           \n",
    "        'GradientBoostingTuningJobName': str,\n",
    "        'RandomForestTuningJobName': str,\n",
    "        'ExtraTreesTuningJobName': str,\n",
    "        'GradientBoostingTransformJobName': str,\n",
    "        'RandomForestTransformJobName': str,\n",
    "        'ExtraTreesTransformJobName': str,\n",
    "        'GradientBoostingModelName': str,\n",
    "        'RandomForestModelName': str,\n",
    "        'ExtraTreesModelName': str,\n",
    "        'GradientBoostingEvaluationJobName': str,\n",
    "        'RandomForestEvaluationJobName': str,\n",
    "        'ExtraTreesEvaluationJobName': str,\n",
    "        'ModelLambdaFunctionName': str, \n",
    "        'EndpointConfigName': str,\n",
    "        'EndpointName': str\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subir scripts a S3\n",
    "==================\n",
    "\n",
    "Para poder agregar la creación de los Jobs de **Amazon SageMaker** para el procesamiento, entrenamiento, evaluación y despliegue; necesitamos subir a **Amazon S3** los scripts necesarios para cada uno de estos procesos. Para esto utilizamos el método [S3Uploader.upload( )](https://sagemaker.readthedocs.io/en/stable/api/utility/s3.html#sagemaker.s3.S3Uploader) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "processing_code_path = S3Uploader.upload(local_path='processing.py',\n",
    "                                            desired_s3_uri='s3://{}/{}'.format(bucket, code_prefix),\n",
    "                                            sagemaker_session=sess)\n",
    "\n",
    "with tarfile.open('train_and_deploy.tar.gz', 'w:gz') as tar:\n",
    "    tar.add('train_and_deploy.py')\n",
    "    \n",
    "train_and_deploy_code_path = S3Uploader.upload(local_path='train_and_deploy.tar.gz',\n",
    "                                            desired_s3_uri='s3://{}/{}'.format(bucket, code_prefix),\n",
    "                                            sagemaker_session=sess)\n",
    "\n",
    "model_evaluation_code_path = S3Uploader.upload(local_path='evaluate_models.py',\n",
    "                                            desired_s3_uri='s3://{}/{}'.format(bucket, code_prefix),\n",
    "                                            sagemaker_session=sess)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-829825986145/churn-clf/code/train_and_deploy.tar.gz'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_deploy_code_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Una vez teniendo los scripts arriba, podemos empezar a crear cada uno de los pasos que agregaremos al workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agregar procesamiento\n",
    "=====================\n",
    "\n",
    "Para crear un primer paso en el workflow, para el pre-procesamiento de los datos, utilizamos el método [steps.sagemaker.ProcessingStep( )](https://aws-step-functions-data-science-sdk.readthedocs.io/en/stable/sagemaker.html?highlight=steps.sagemaker.ProcessingStep#stepfunctions.steps.sagemaker.ProcessingStep)\n",
    "\n",
    "Proporcionando los siguientes parámetros:\n",
    "\n",
    "*   `state_id` – Identificador del estado (paso) del workflow, este aparecerá como el nombre en el diagrama del workflow\n",
    "*   `processor` – El processor que será utilizado para ejecutar el job, en este caso el que fue creado en [3.4 Crear Processing Job](/es/sagemaker/dataprep/processingjob)\n",
    "*   `job_name` – Nombre del job a crear, en este caso será el valor recibido en le parámetro `ProcessingJobName` del workflow\n",
    "*   `inputs` – Rutas de **Amazon S3** de las cuales se tomarán los archivos de entrada para el proceso\n",
    "*   `outputs` – Ruta de **Amazon S3** en la cual queremos que se deposite el resultado de la ejecución del proceso\n",
    "*   `container_arguments` – Parámetros que recibirá el container en el cual se ejecutará el script\n",
    "*   `container_entrypoint` – Ruta del script a ejecutar dentro del container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_images = {'Processing': {'libraries': {'pandas': '1.2.4',\n",
    "   'numpy': '1.20.2',\n",
    "   'scikit-learn': '0.24.2'},\n",
    "  'build_id': 'churn-clf-processing-build-image:7e3a3c1e-4038-4a84-aae6-408606f73789',\n",
    "  'image_uri': '829825986145.dkr.ecr.us-east-1.amazonaws.com/churn-clf-processing:latest'},\n",
    " 'Training': {'libraries': {'pandas': '1.2.4',\n",
    "   'numpy': '1.20.2',\n",
    "   'scikit-learn': '0.24.2',\n",
    "   'sagemaker-training': '3.9.2'},\n",
    "  'build_id': 'churn-clf-training-build-image:acd0f05b-4dba-48d4-85ca-c06c2addfd4e',\n",
    "  'image_uri': '829825986145.dkr.ecr.us-east-1.amazonaws.com/churn-clf-training:latest'},\n",
    " 'Inference': {'libraries': {'pandas': '1.2.4',\n",
    "   'numpy': '1.20.2',\n",
    "   'scikit-learn': '0.24.2',\n",
    "   'multi-model-server': '1.1.8',\n",
    "   'sagemaker-inference': '1.5.11',\n",
    "   'boto3': '1.21.43',\n",
    "   'itsdangerous': '2.0.1'},\n",
    "  'dependencies': [('serving', '/opt/ml/serving')],\n",
    "  'others': ['RUN pip install -e /opt/ml/serving',\n",
    "   'LABEL com.amazonaws.sagemaker.capabilities.multi-models=false',\n",
    "   'LABEL com.amazonaws.sagemaker.capabilities.accept-bind-to-port=true'],\n",
    "  'entrypoint': ['python', '/opt/ml/serving/custom_inference/serving.py'],\n",
    "  'cmd': ['serve'],\n",
    "  'build_id': 'churn-clf-inference-build-image:c096f995-2a79-4ea7-8364-0aa4afa43752',\n",
    "  'image_uri': '829825986145.dkr.ecr.us-east-1.amazonaws.com/churn-clf-inference:latest'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_script_file = 'code/processing.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor(\n",
    "    image_uri=docker_images['Processing']['image_uri'],\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    entrypoint=['python3',f'/opt/ml/processing/input/code/{os.path.basename(data_prep_script_file)}'],\n",
    "    volume_size_in_gb=5,\n",
    "    max_runtime_in_seconds=60*60*2)# dos horas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_step = steps.sagemaker.ProcessingStep(\n",
    "state_id= 'Preparación de Datos',\n",
    "processor= processor,\n",
    "job_name=execution_input['ProcessingJobName'],\n",
    "inputs=[ProcessingInput(input_name='input',\n",
    "                        source='s3://{}/{}'.format(bucket, datasets_prefix), \n",
    "                        destination='/opt/ml/processing/input'),\n",
    "        ProcessingInput(input_name='code',\n",
    "                        source=processing_code_path, \n",
    "                        destination='/opt/ml/processing/input/code')],\n",
    "outputs=[ProcessingOutput(output_name='output',\n",
    "                            source='/opt/ml/processing/output/data',\n",
    "                            destination='s3://{}/{}'.format(bucket, processed_data_prefix))],\n",
    "container_arguments=['--test-size', '0.1',\n",
    "                        '--data-file', 'churn.txt',\n",
    "                        '--train-data-file', train_data_file,\n",
    "                        '--train-target-file', train_target_file,\n",
    "                        '--test-data-file', test_data_file,\n",
    "                        '--test-target-file', test_target_file,\n",
    "                        '--encoder-file', encoder_file],\n",
    "container_entrypoint=['python3','/opt/ml/processing/input/code/processing.py'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función registrar modelo\n",
    "========================\n",
    "\n",
    "Antes de agregar el entrenamiento del modelo al worflow, crearemos una función en **AWS Lambda** que nos permitirá registrar en **Amazon SageMaker** el modelo que mejor desempeño haya tenido después de realizar la optimización de hiperparámetros.\n",
    "\n",
    "Para esto primero debemos crear el script con la lógica que esta función ejecutará. En este caso utilizaremos el método [create\\_model( )](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model) de `boto3` (SDK de AWS para Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf lambda/register_model/; mkdir lambda/register_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing lambda/register_model/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lambda/register_model/app.py\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "def get_parameter(key, event):\n",
    "    parameter_value = None\n",
    "    if (key in event):\n",
    "        parameter_value = event[key]\n",
    "\n",
    "    else:\n",
    "        raise KeyError('{} key not found in function input!'.format(key)+\n",
    "                      ' The input received was: {}.'.format(event))\n",
    "        \n",
    "    return parameter_value\n",
    "    \n",
    "\n",
    "#Retrieve training job name from event and create a model.\n",
    "def lambda_handler(event, context):\n",
    "    response = None\n",
    "    \n",
    "    print(event)\n",
    "    \n",
    "    job_name = get_parameter('TrainingJobName', event)\n",
    "    model_name = get_parameter('ModelName', event)\n",
    "    code_path = get_parameter('CodePath', event)\n",
    "        \n",
    "    try:\n",
    "        \n",
    "        response = sm_client.describe_training_job(TrainingJobName=job_name)\n",
    "        model_data = response['ModelArtifacts']['S3ModelArtifacts']\n",
    "        container = response['AlgorithmSpecification']['TrainingImage']\n",
    "        role_arn = response['RoleArn']\n",
    "        \n",
    "        print(\"Training job: {} has artifacts at: {}.\".format(job_name, model_data))\n",
    "        \n",
    "        try:\n",
    "            response = sm_client.create_model(ModelName=model_name,\n",
    "                                   PrimaryContainer={\n",
    "                                       'Image':container,\n",
    "                                       'Mode':'SingleModel',\n",
    "                                       'ModelDataUrl':model_data,\n",
    "                                       'Environment':{\n",
    "                                           'SAGEMAKER_PROGRAM': 'train_and_deploy.py',\n",
    "                                           'SAGEMAKER_SUBMIT_DIRECTORY': code_path\n",
    "                                       }\n",
    "                                   },\n",
    "                                   ExecutionRoleArn=role_arn\n",
    "            )\n",
    "\n",
    "            print(response)\n",
    "\n",
    "        except Exception as e:\n",
    "            response = ('Failed to create model')\n",
    "            print(e)\n",
    "            print('{} Attempted to create a model for job name: {}.'.format(response, job_name))\n",
    "\n",
    "    except Exception as e:\n",
    "        response = ('Failed to read training job artifacts!'+ \n",
    "                    ' The training job may not exist or the job name may be incorrect.'+ \n",
    "                    ' Check SageMaker to confirm the job name.')\n",
    "        print(e)\n",
    "        print('{} Attempted to read job name: {}.'.format(response, job_name))\n",
    "            \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'ModelArn': response['ModelArn'],\n",
    "        'TrainingJobName': job_name\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de contenedor para función Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear los contenedores Docker utilizaremos el servicio AWS Code Build y debido a que las imágenes bases serán descargadas del DockerHub repository podríamos llegar a obtener un error indicando que se han realizado demasiadas solicitudes, para conocer más detalles de esta limitante visitar este enlace.\n",
    "\n",
    "Para evitar ese error necesitamos autenticarnos y para esto debemos obtener una cuenta en DockerHub y sustituir usuario y constraseña por los valores correspondientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_name = 'dockerhub'\n",
    "#sagemaker_utils.create_secret(secret_name,'user','pwd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitaremos un rol de ejecución para ser utilizado en el proyecto de AWS Code Build. Si estamos ejecutando el Notebook con permisos suficientes para crear un rol de IAM, podemos crear el rol simplemente ejecutando el siguiente método, de lo contrario tendría que ser creado de forma manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_document={\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [               \n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"ecr:BatchCheckLayerAvailability\",\n",
    "                    \"ecr:CompleteLayerUpload\",\n",
    "                    \"ecr:GetAuthorizationToken\",\n",
    "                    \"ecr:InitiateLayerUpload\",\n",
    "                    \"ecr:PutImage\",\n",
    "                    \"ecr:UploadLayerPart\",\n",
    "                    \"ecr:BatchGetImage\",\n",
    "                    \"ecr:GetDownloadUrlForLayer\",\n",
    "                    \"logs:CreateLogGroup\",\n",
    "                    \"logs:CreateLogStream\",\n",
    "                    \"logs:PutLogEvents\",\n",
    "                    \"s3:PutObject\",\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:GetObjectVersion\",\n",
    "                    \"secretsmanager:GetSecretValue\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "#codebuild_role = sagemaker_utils.create_codebuild_execution_role('CodeBuildExecutionRole', policy_document)\n",
    "codebuild_role = \"arn:aws:iam::829825986145:role/CodeBuildExecutionRole\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::829825986145:role/CodeBuildExecutionRole'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codebuild_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Repository churn-clf-lambda already exists\n",
      "lambda/register_model/\n",
      "\u001b[Kchurn-clf-lambda................................SUCCEEDED\n",
      "\u001b[K\u001b[34m✅\u001b[0m Building docker image\n"
     ]
    }
   ],
   "source": [
    "docker_images['Lambda']={'libraries':{'boto3':'1.17.77'},\n",
    "                         'dependencies':[('lambda/register_model/','${LAMBDA_TASK_ROOT}')],                                                  \n",
    "                         'cmd':['app.lambda_handler'],\n",
    "                         'lambda': True,}\n",
    "\n",
    "lambda_docker_parameters = {'image_name': f'{prefix}-lambda',\n",
    "                            'base_image': \"public.ecr.aws/lambda/python:3.8\",\n",
    "                            's3_path': f's3://{bucket}/{images_directory}',\n",
    "                            'role': codebuild_role,  \n",
    "                            #'secret': secret_name\n",
    "                            }\n",
    "\n",
    "lambda_docker_parameters.update(docker_images['Lambda'])\n",
    "docker_images['Lambda']['image_uri'] = sagemaker_utils.create_docker_image(**lambda_docker_parameters)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de función\n",
    "===================\n",
    "\n",
    "Obtener nombres de columnas del dataset para poder ordenarlas en la función\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Role already exists, updating it...\n",
      "INFO: Role updated: churn-clf-LambdaExecutionRole\n"
     ]
    }
   ],
   "source": [
    "policy_document = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"sagemaker:*\",\n",
    "                    \"s3:*\",\n",
    "                    \"states:StartExecution\",\n",
    "                    \"iam:PassRole\",\n",
    "                    \"logs:CreateLogGroup\",\n",
    "                    \"logs:CreateLogStream\",\n",
    "                    \"logs:PutLogEvents\"\n",
    "                ],\n",
    "                \"Resource\": \"*\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "lambda_role = sagemaker_utils.create_lambda_execution_role(f'{prefix}-LambdaExecutionRole', policy_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_function_name = f'{prefix}-register-model'\n",
    "lambda_function_params = {\n",
    "    'FunctionName': endpoint_function_name,\n",
    "    'Role': lambda_role,\n",
    "    'Code': {'ImageUri':docker_images['Lambda']['image_uri']},\n",
    "    'Description': 'Queries a SageMaker training job and creates a model.',\n",
    "    'Timeout': 15, # 5 mins\n",
    "    'MemorySize': 128, # MB\n",
    "    'PackageType': 'Image'#,\n",
    "    #'Environment': {\n",
    "    #    'Variables': {\n",
    "    #        'ENDPOINT_NAME': 'Colocar aqui el nombre del endpoint',\n",
    "    #        'ENCODER_S3_PATH': f\"{sagemaker_utils.get_processor_output_path(processor,'encoder')}/{encoder_file}\",\n",
    "    #        'THRESHOLDS_S3_PATH': f\"{sagemaker_utils.get_processor_output_path(evaluation_processor,'eval')}/{thresholds_file}\"\n",
    "    #    }\n",
    "    #}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "lambda_response = sagemaker_utils.create_lambda_function(**lambda_function_params)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4989b284843198940c9466d0e8ffb2ed3dc06eb1954444794d5f5c9e3a6f8233"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
