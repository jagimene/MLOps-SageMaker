{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capítulo 6\n",
    "\n",
    "Optimización de hiperparámetros\n",
    "===============================\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Amazon SageMaker nos ofrece la funcionalidad de optimización de hiperpárametros, ya sea utilizando una búsqueda aleatoria o un método bayesiano, en este caso vamos a utilizar el segundo método el cual permite entrenar un modelo de forma iterativa e ir identificando que combinación de hiperparámetros, nos permite minimizar o maximizar más la métrica objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como funciona la optimización de hiperparámetros en SageMaker\n",
    "=============================================================\n",
    "\n",
    "**Amazon SageMaker** nos permite encontrar la mejor combinación de valores de los hiperparámetros de manera automática, mediante la ejecución de múltiples procesos de entrenamiento utilizando el rango de valores de los hiperparámetros que se especifique. Posteriormente selecciona los valores de hiperparámetros que resulten en el modelo con mejor desempeño, con base en una métrica objetivo previamente seleccionada.\n",
    "\n",
    "Existen dos métodos que podemos utilizar para la búsqueda de los mejores valores de hiperparámetros:\n",
    "\n",
    "*   **Random Search** – en la búsqueda aleatoria, **Amazon SageMaker** selecciona una combinación de valores de los rangos, especificados para los hiperparámetros, de forma aleatoria y posteriormente selecciona aquella combinación que haya dado el mejor desempeño. Debido a que la selección de la combinación de valores de los hiperparámetros no depende del resultado de los procesos de entrenamiento previos, es posible ejecutar el máximo número de procesos concurrentes en paralelo sin afectar el desempeño de la búsqueda\n",
    "    \n",
    "*   **Bayesian Search** – la búsqueda bayesiana trata la selección de los valores de los hiperparámetros como si se tratará de un problema de regresión. Dado un conjunto de hiperparámetros, **Amazon SageMaker** estima que combinación de estos tienen mayor probabilidad de obtener los mejores resultados y ejecuta procesos de entrenamiento para comprobar eso. Después de probar el primer conjunto de valores de hiperparámetros, vuelve a estimar la mejor combinación para la siguiente prueba. Continua de esa forma hasta alcanzar el número máximo de procesos de entrenamiento especificados al lanzar el proceso de optimización\n",
    "    \n",
    "\n",
    "En este taller utilizaremos el método de búsqueda bayesiana para la optmización de los valores de los hiperparámetros.\n",
    "\n",
    "Para conocer más de la funcionalidad de optimización de hiperparámetros de **Amazon SageMaker** consultar la [documentación](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear proceso de optimización (Tuning Job)\n",
    "==========================================\n",
    "\n",
    "Gracias a que creamos tres estimators para el entrenamiento, ahora podemos crear tres procesos de optimización que se ejecuten en paralelo, para esto utilizamos la clase `HyperparameterTuner` y especificamos los rangos de valores para los hiperparámetros utilizando `ContinuousParameter`, `IntegerParameter` o `CategoricalParameter` dependiendo de si se trata de un valor continuo, discreto o categórico; respectivamente.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'estimators' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=2'>3</a>\u001b[0m total_jobs \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=3'>4</a>\u001b[0m parallel_jobs \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=5'>6</a>\u001b[0m tuners[\u001b[39m'\u001b[39m\u001b[39mGradientBoosting\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m HyperparameterTuner(\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=6'>7</a>\u001b[0m         estimator\u001b[39m=\u001b[39mestimators[\u001b[39m'\u001b[39m\u001b[39mGradientBoosting\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=7'>8</a>\u001b[0m         objective_metric_name\u001b[39m=\u001b[39mmetric_name,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=8'>9</a>\u001b[0m         objective_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMaximize\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=9'>10</a>\u001b[0m         hyperparameter_ranges\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mlearning-rate\u001b[39m\u001b[39m'\u001b[39m: ContinuousParameter(\u001b[39m0.01\u001b[39m, \u001b[39m0.1\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=10'>11</a>\u001b[0m                                 \u001b[39m'\u001b[39m\u001b[39mmin-samples-split\u001b[39m\u001b[39m'\u001b[39m: IntegerParameter(\u001b[39m5\u001b[39m, \u001b[39m15\u001b[39m), \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=11'>12</a>\u001b[0m                                 \u001b[39m'\u001b[39m\u001b[39mn-estimators\u001b[39m\u001b[39m'\u001b[39m: IntegerParameter(\u001b[39m400\u001b[39m, \u001b[39m800\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=12'>13</a>\u001b[0m                                 \u001b[39m'\u001b[39m\u001b[39mmax-depth\u001b[39m\u001b[39m'\u001b[39m: IntegerParameter(\u001b[39m3\u001b[39m, \u001b[39m10\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=13'>14</a>\u001b[0m                                 \u001b[39m'\u001b[39m\u001b[39mmax-features\u001b[39m\u001b[39m'\u001b[39m: IntegerParameter(\u001b[39m15\u001b[39m, \u001b[39m30\u001b[39m)},\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=14'>15</a>\u001b[0m         metric_definitions\u001b[39m=\u001b[39m[{\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m: metric_name, \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=15'>16</a>\u001b[0m                                 \u001b[39m'\u001b[39m\u001b[39mRegex\u001b[39m\u001b[39m'\u001b[39m: metric_regex}],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=16'>17</a>\u001b[0m         max_jobs\u001b[39m=\u001b[39mtotal_jobs,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=17'>18</a>\u001b[0m         max_parallel_jobs\u001b[39m=\u001b[39mparallel_jobs)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=19'>20</a>\u001b[0m tuners[\u001b[39m'\u001b[39m\u001b[39mRandomForest\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m HyperparameterTuner(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=20'>21</a>\u001b[0m         estimator\u001b[39m=\u001b[39mestimators[\u001b[39m'\u001b[39m\u001b[39mRandomForest\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=21'>22</a>\u001b[0m         objective_metric_name\u001b[39m=\u001b[39mmetric_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=29'>30</a>\u001b[0m         max_jobs\u001b[39m=\u001b[39mtotal_jobs,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=30'>31</a>\u001b[0m         max_parallel_jobs\u001b[39m=\u001b[39mparallel_jobs)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=32'>33</a>\u001b[0m tuners[\u001b[39m'\u001b[39m\u001b[39mExtraTrees\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m HyperparameterTuner(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=33'>34</a>\u001b[0m         estimator\u001b[39m=\u001b[39mestimators[\u001b[39m'\u001b[39m\u001b[39mExtraTrees\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=34'>35</a>\u001b[0m         objective_metric_name\u001b[39m=\u001b[39mmetric_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=42'>43</a>\u001b[0m         max_jobs\u001b[39m=\u001b[39mtotal_jobs,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/ch/vscode/ch/repos/mlops-sagemaker/lib/scripts/notebook/sagemaker_pipelines/hiperparameter_optimization/hiperparameter_optimization.ipynb#ch0000005vscode-remote?line=43'>44</a>\u001b[0m         max_parallel_jobs\u001b[39m=\u001b[39mparallel_jobs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'estimators' is not defined"
     ]
    }
   ],
   "source": [
    "tuners = {}\n",
    "\n",
    "total_jobs = 16\n",
    "parallel_jobs = 2\n",
    "\n",
    "tuners['GradientBoosting'] = HyperparameterTuner(\n",
    "        estimator=estimators['GradientBoosting'],\n",
    "        objective_metric_name=metric_name,\n",
    "        objective_type='Maximize',\n",
    "        hyperparameter_ranges={'learning-rate': ContinuousParameter(0.01, 0.1),\n",
    "                                'min-samples-split': IntegerParameter(5, 15), \n",
    "                                'n-estimators': IntegerParameter(400, 800),\n",
    "                                'max-depth': IntegerParameter(3, 10),\n",
    "                                'max-features': IntegerParameter(15, 30)},\n",
    "        metric_definitions=[{'Name': metric_name, \n",
    "                                'Regex': metric_regex}],\n",
    "        max_jobs=total_jobs,\n",
    "        max_parallel_jobs=parallel_jobs)\n",
    "\n",
    "tuners['RandomForest'] = HyperparameterTuner(\n",
    "        estimator=estimators['RandomForest'],\n",
    "        objective_metric_name=metric_name,\n",
    "        objective_type='Maximize',\n",
    "        hyperparameter_ranges={'min-samples-split': IntegerParameter(3,10), \n",
    "                                'n-estimators': IntegerParameter(150,300),\n",
    "                                'max-depth': IntegerParameter(20,35),\n",
    "                                'max-features': IntegerParameter(15,30)},\n",
    "        metric_definitions=[{'Name': metric_name, \n",
    "                                'Regex': metric_regex}],\n",
    "        max_jobs=total_jobs,\n",
    "        max_parallel_jobs=parallel_jobs)\n",
    "\n",
    "tuners['ExtraTrees'] = HyperparameterTuner(\n",
    "        estimator=estimators['ExtraTrees'],\n",
    "        objective_metric_name=metric_name,\n",
    "        objective_type='Maximize',\n",
    "        hyperparameter_ranges={'min-samples-split': IntegerParameter(3,10), \n",
    "                                'n-estimators': IntegerParameter(150,350),\n",
    "                                'max-depth': IntegerParameter(20,35),\n",
    "                                'max-features': IntegerParameter(15,30)},\n",
    "        metric_definitions=[{'Name': metric_name, \n",
    "                                'Regex': metric_regex}],\n",
    "        max_jobs=total_jobs,\n",
    "        max_parallel_jobs=parallel_jobs)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `total_jobs` especifíca el número total de procesos de entrenamiento (combinaciones distintas de valores de hiperparámetros) a ejecutar y la variable `parallel_jobs` especifíca el número máximo de procesos a ejecutar en paralelo.\n",
    "\n",
    "Para ejecutar cada uno de los procesos utilizamos el método `fit()` pasando como parámetros la ubicación de los datasets y mediante el parámetro `wait=False` es que le indicamos que ejecute el proceso de manera asíncrona (sin esperar a que cada uno de los procesos termine).\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for tuner in tuners:\n",
    "        tuners[tuner].fit({'train_data': sagemaker_utils.get_processor_output_path(processor, 'train_data'),\n",
    "                           'train_target': sagemaker_utils.get_processor_output_path(processor, 'train_target')}, \n",
    "                          job_name= f'{prefix}-{tuner}-{strftime(\"%M-%S\", gmtime())}',\n",
    "                          wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que los tres procesos se ejecutan de forma asíncrona y paralela, mediante el siguiente método podemos esperar a que los tres procesos hayan terminado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_utils.wait_for_optmimization_jobs(tuners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante la invocación del método `describe()` podemos obtener el proceso de entrenamiento cuyo modelo resultante obtuvo el mejor desempeño, en este caso maximizando la métrica objetivo _Recall_. Posteriormente de los metadatos devueltos del mejor proceso de entrenamiento, podemos obtener los valores de los hiperparámetros así cómo el valor obtenido en la métrica objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "    for tuner in tuners:\n",
    "        best_training_job = tuners[tuner].describe()['BestTrainingJob']\n",
    "        objective_metric = best_training_job['FinalHyperParameterTuningJobObjectiveMetric']\n",
    "        \n",
    "        hyperparameters[tuner] = best_training_job['TunedHyperParameters']\n",
    "        print(tuner)\n",
    "        print(f\"\\thyper parameters: {hyperparameters[tuner]}\")\n",
    "        print(f\"\\t{objective_metric['MetricName']}: {objective_metric['Value']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos el mejor candidato (modelo con mejor desempeño) para cada uno de los tres algoritmos, podemos pasar a comparar el desempeño entre estos para seleccionar el mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4989b284843198940c9466d0e8ffb2ed3dc06eb1954444794d5f5c9e3a6f8233"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
